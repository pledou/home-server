services:

  webui:
    labels:
      - traefik.enable=true
      - traefik.docker.network=traefik_backend
      - traefik.constraint-label=traefik_backend
      - traefik.http.routers.ia-https.rule=Host(`ia.{{ app_domain_name }}`)
      - traefik.http.routers.ia-https.entrypoints=https
      - traefik.http.routers.ia-https.tls=true
      - traefik.http.routers.ia-https.tls.certresolver=le
      - traefik.http.services.ia.loadbalancer.server.port=8080
    image: ghcr.io/open-webui/open-webui:main
    environment:
      OLLAMA_BASE_URL: http://ollama:11434

      # Generic OAuth
      OAUTH_PROVIDER_NAME: "authentik"
      OAUTH_CLIENT_ID: "{{ webui_oauth_client_id }}"
      OAUTH_CLIENT_SECRET: "{{ webui_oauth_client_secret }}"
      OAUTH_SCOPES: "openid profile email"
      OPENID_PROVIDER_URL: "https://auth.{{ app_domain_name }}/application/o/{{ webui_oauth_slug_name }}/.well-known/openid-configuration"
      ENABLE_OAUTH_SIGNUP: True
      OAUTH_MERGE_ACCOUNTS_BY_EMAIL: True

      OPENAI_API_BASE_URL: "http://pipelines:9099/v1"
      OPENAI_API_KEY: "0p3n-w3bu!"
      AUDIO_OPENAI_API_BASE_URL: "http://speech:8000/v1"
      AUDIO_OPENAI_API_MODEL: "tts-1"
      AUDIO_OPENAI_API_SPEAKER: "upmc"
      AUDIO_OPENAI_API_KEY: "openai-api-key"
      WHISPER_MODEL_AUTO_UPDATE: True
      WHISPER_MODEL_AUTO_UPDATE_INTERVAL: 2w
      ENABLE_ADMIN_EXPORT: False
    restart: always
    volumes:
      - open-webui:/app/backend/data
    networks:
      - network
      - traefik_backend
    depends_on:
     - ollama

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    networks:
      network:
        aliases:
          - pipelines
    volumes:
      - pipelines:/app/pipelines
    restart: always

  ollama:
    image: ollama/ollama
    expose:
     - 11434/tcp
    ports:
     - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_THREADS=10
      - OLLAMA_HOST=0.0.0.0
    networks:
      network:
        aliases:
          - ollama
    volumes:
      - ollama:/root/.ollama
    
    mem_limit: "32G"
    shm_size: "16g" 

    # for nvidea gpu
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           device_ids: ['all']
    #           capabilities: [gpu]

  speech:
    image: ghcr.io/matatonic/openedai-speech
    environment:
      - TTS_HOME=voices
      - HF_HOME=voices
      #PRELOAD_MODEL=xtts
      #PRELOAD_MODEL=xtts_v2.0.2
      #PRELOAD_MODEL=parler-tts/parler_tts_mini_v0.1
    # ports:
    #   - "8000:8000"
    networks:
      network:
        aliases:
          - speech
    volumes:
      - voices:/app/voices
      - speech_config:/app/config
    restart: unless-stopped
    # Set nvidia runtime if it's not the default
    #runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           #device_ids: ['0', '1'] # Select a gpu, or
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama:
  open-webui:
  voices:
   driver: local
   driver_opts:
     type: none
     o: bind
     device: "{{ docker_volumes_path }}/voices"
  pipelines:
  speech_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/speech"
  searxng:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/searxng"

networks:
  network:
  traefik_backend:
    external: true