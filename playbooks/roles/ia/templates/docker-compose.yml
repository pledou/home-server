services:

  webui:
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - traefik.enable=true
      - traefik.docker.network=traefik_backend
      - traefik.constraint-label=traefik_backend
      - traefik.http.routers.ia-https.rule=Host(`ia.{{ app_domain_name }}`)
      - traefik.http.routers.ia-https.entrypoints=https
      - traefik.http.routers.ia-https.tls=true
      - traefik.http.routers.ia-https.tls.certresolver=le
      - traefik.http.services.ia.loadbalancer.server.port=8080
    image: ghcr.io/open-webui/open-webui:{{ webui_version }}
    # Dependencies for tools
    command: >
      bash -c "
        pip install requests pydantic caldav icalendar &&
        exec bash start.sh
      "
    environment:
      OLLAMA_BASE_URL: http://ollama:11434

      # Generic OAuth
      OAUTH_PROVIDER_NAME: "authentik"
      OAUTH_CLIENT_ID: "{{ webui_oauth_client_id }}" # TODO move to secret
      OAUTH_CLIENT_SECRET: "{{ webui_oauth_client_secret }}" # TODO move to secret
      OAUTH_SCOPES: "openid profile email"
      OPENID_PROVIDER_URL: "https://auth.{{ app_domain_name }}/application/o/{{ webui_oauth_slug_name }}/.well-known/openid-configuration"
      WEBUI_URL: "https://ia.{{ app_domain_name }}"
      ENABLE_OAUTH_SIGNUP: True
      ENABLE_LOGIN_FORM: False
      DEFAULT_LOCALE: "fr"
      DEFAULT_MODELS: "mixtral-8x7b"
      DEFAULT_USER_ROLE: "user"
      OAUTH_MERGE_ACCOUNTS_BY_EMAIL: True

      AUDIO_OPENAI_API_BASE_URL: "http://speech:8000/v1"
      AUDIO_OPENAI_API_MODEL: "tts-1"
      AUDIO_OPENAI_API_SPEAKER: "upmc"
      AUDIO_OPENAI_API_KEY: "openai-api-key"
      WHISPER_MODEL_AUTO_UPDATE: True
      WHISPER_MODEL_AUTO_UPDATE_INTERVAL: 2w
      ENABLE_ADMIN_EXPORT: False
    restart: always
    volumes:
      - open-webui:/app/backend/data
      - ./install-tools.sh:/install-tools.sh:ro
    networks:
      - network
      - traefik_backend
    depends_on:
     - ollama


  {% if gpu != 'intel' -%}
  ollama:
    labels:
      - com.centurylinklabs.watchtower.enable=true
    image: ollama/ollama:{{ ollama_version }}
    expose:
     - 11434/tcp
    ports:
     - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_THREADS=10
      - OLLAMA_HOST=0.0.0.0
    networks:
      network:
        aliases:
          - ollama
    volumes:
      - ollama:/root/.ollama
    
    mem_limit: "32G"
    shm_size: "16g" 

    {% if gpu == 'nvidia' -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]
    {%- endif %}
  {% else -%}
  ollama:
    labels:
      - com.centurylinklabs.watchtower.enable=true
    user: root
    image: intelanalytics/ipex-llm-inference-cpp-xpu:{{ ollama_intel_version }}
    mem_limit: "32G"
    shm_size: "16g" 
    pull_policy: always
    tty: true
    restart: always
    expose:
    - 11434/tcp
    ports:
    - 11434:11434/tcp
    # healthcheck:
    #   test: ollama --version || exit 1
    environment:
      - DEVICE=iGPU
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_THREADS=10
      - OLLAMA_HOST=0.0.0.0
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - SYCL_CACHE_PERSISTENT=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
    working_dir: /llm/scripts/
    command: ["bash", "-c", "source ipex-llm-init --gpu --device iGPU && bash start-ollama.sh"]
    {#- command: ["bash", "-c", "source ipex-llm-init --gpu --device iGPU && tail -F anything"] # debuggage #}
    
    volumes:
      - ollama:/root/.ollama
    devices:
      - /dev/dri:/dev/dri
    networks:
      network:
        aliases:
          - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: intel
              device_ids: ['all']
              capabilities: [gpu]
  {%- endif %}

  speech:
    labels:
      - com.centurylinklabs.watchtower.enable=true
    image: ghcr.io/matatonic/openedai-speech:{{ speech_version }}
    environment:
      - TTS_HOME=voices
      - HF_HOME=voices
      #PRELOAD_MODEL=xtts
      #PRELOAD_MODEL=xtts_v2.0.2
      #PRELOAD_MODEL=parler-tts/parler_tts_mini_v0.1
    # ports:
    #   - "8000:8000"
    networks:
      network:
        aliases:
          - speech
    volumes:
      - voices:/app/voices
      - speech_config:/app/config
    restart: unless-stopped
    {%- if gpu == 'nvidia' %}
    {#- Set nvidia runtime if it's not the default #}
    runtime: nvidia
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               # device_ids: ['0', '1'] # Select a gpu, or use 'all' to use all available gpus
               count: all
               capabilities: [gpu]
    {%- endif %}

volumes:
  ollama:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/ollama"
  open-webui:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/open-webui"
  voices:
   driver: local
   driver_opts:
     type: none
     o: bind
     device: "{{ docker_volumes_path }}/voices"
  speech_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/speech"

networks:
  network:
  traefik_backend:
    external: true