services:

  webui:
    labels:
      - traefik.enable=true
      - traefik.docker.network=traefik_backend
      - traefik.constraint-label=traefik_backend
      - traefik.http.routers.ia-https.rule=Host(`ia.{{ app_domain_name }}`)
      - traefik.http.routers.ia-https.entrypoints=https
      - traefik.http.routers.ia-https.tls=true
      - traefik.http.routers.ia-https.tls.certresolver=le
      - traefik.http.services.ia.loadbalancer.server.port=8080
      - traefik.http.routers.ia-https.middlewares=authentik@docker
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-authentik-email
      - WEBUI_AUTH_TRUSTED_GROUPS_HEADER=X-authentik-groups
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=X-authentik-name
      - WEBUI_AUTH_TRUSTED_UID_HEADER=X-authentik-uid
      - WEBUI_AUTH_TRUSTED_USERNAME_HEADER=X-authentik-username
      - WEBUI_AUTH_TRUSTED_JWT_HEADER=X-authentik-jwt
      - WEBUI_AUTH_TRUSTED_META_JWKS_HEADER=X-authentik-meta-jwks
      - WEBUI_AUTH_TRUSTED_META_OUTPOST_HEADER=X-authentik-meta-outpost
      - WEBUI_AUTH_TRUSTED_META_PROVIDER_HEADER=X-authentik-meta-provider
      - WEBUI_AUTH_TRUSTED_META_APP_HEADER=X-authentik-meta-app
      - WEBUI_AUTH_TRUSTED_META_VERSION_HEADER=X-authentik-meta-version
      - OPENAI_API_BASE_URL="http://pipelines:9099/v1"
      - OPENAI_API_KEY="0p3n-w3bu!"
      - AUDIO_OPENAI_API_BASE_URL="http://speech:8000/v1"
      - AUDIO_OPENAI_API_MODEL="tts-1"
      - AUDIO_OPENAI_API_SPEAKER="upmc"
      - AUDIO_OPENAI_API_KEY="openai-api-key"
      - WHISPER_MODEL_AUTO_UPDATE=True
      - WHISPER_MODEL_AUTO_UPDATE_INTERVAL=2w
      - ENABLE_ADMIN_EXPORT=False
    restart: always
    volumes:
      - open-webui:/app/backend/data
    networks:
      - network
      - traefik_backend
    depends_on:
     - ollama

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    networks:
      network:
        aliases:
          - pipelines
    volumes:
      - pipelines:/app/pipelines
    restart: always

  ollama:
    image: ollama/ollama
    expose:
     - 11434/tcp
    ports:
     - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_THREADS=10
      - OLLAMA_HOST=0.0.0.0
    networks:
      network:
        aliases:
          - ollama
    volumes:
      - ollama:/root/.ollama
    
    mem_limit: "32G"
    shm_size: "16g" 

    # for nvidea gpu
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           device_ids: ['all']
    #           capabilities: [gpu]

  speech:
    image: ghcr.io/matatonic/openedai-speech
    environment:
      - TTS_HOME=voices
      - HF_HOME=voices
      #PRELOAD_MODEL=xtts
      #PRELOAD_MODEL=xtts_v2.0.2
      #PRELOAD_MODEL=parler-tts/parler_tts_mini_v0.1
    # ports:
    #   - "8000:8000"
    networks:
      network:
        aliases:
          - speech
    volumes:
      - voices:/app/voices
      - speech_config:/app/config
    restart: unless-stopped
    # Set nvidia runtime if it's not the default
    #runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           #device_ids: ['0', '1'] # Select a gpu, or
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama:
  open-webui:
  voices:
   driver: local
   driver_opts:
     type: none
     o: bind
     device: "{{ docker_volumes_path }}/voices"
  pipelines:
  speech_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/speech"
  searxng:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "{{ docker_volumes_path }}/searxng"

networks:
  network:
  traefik_backend:
    external: true